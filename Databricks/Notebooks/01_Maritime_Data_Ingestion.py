# Databricks notebook source
# MAGIC %md
# MAGIC # Maritime Data Ingestion Pipeline
# MAGIC ## Ingest historical vessel and sensor data into Delta Lake
# MAGIC 
# MAGIC **Features:**
# MAGIC - Read from Azure Event Hubs / Kafka
# MAGIC - Schema validation and data quality checks
# MAGIC - Write to Delta Lake with partitioning
# MAGIC - Support for batch and streaming ingestion

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Configuration and Setup

# COMMAND ----------

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from delta.tables import *
import json

# Initialize Spark with Delta Lake
spark = SparkSession.builder \
    .appName("MaritimeIQ-DataIngestion") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()

# Set logging level
spark.sparkContext.setLogLevel("WARN")

print("‚úÖ Spark session initialized with Delta Lake support")
print(f"Spark version: {spark.version}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Define Schemas

# COMMAND ----------

# AIS Vessel Position Schema
ais_schema = StructType([
    StructField("VesselName", StringType(), False),
    StructField("MMSI", StringType(), False),
    StructField("Latitude", DoubleType(), False),
    StructField("Longitude", DoubleType(), False),
    StructField("Speed", DoubleType(), True),
    StructField("Heading", IntegerType(), True),
    StructField("Timestamp", TimestampType(), False)
])

# Environmental Sensor Schema
environmental_schema = StructType([
    StructField("VesselId", IntegerType(), False),
    StructField("MeasurementTime", TimestampType(), False),
    StructField("CO2EmissionKg", DoubleType(), True),
    StructField("NOxEmissionKg", DoubleType(), True),
    StructField("SOxEmissionKg", DoubleType(), True),
    StructField("FuelConsumptionLiters", DoubleType(), True),
    StructField("BatteryStateOfCharge", DoubleType(), True),
    StructField("WaterTemperature", DoubleType(), True),
    StructField("AirTemperature", DoubleType(), True)
])

# Voyage Data Schema
voyage_schema = StructType([
    StructField("VoyageId", StringType(), False),
    StructField("VesselId", IntegerType(), False),
    StructField("DeparturePort", StringType(), True),
    StructField("ArrivalPort", StringType(), True),
    StructField("DepartureTime", TimestampType(), True),
    StructField("ArrivalTime", TimestampType(), True),
    StructField("Status", StringType(), True),
    StructField("PassengerCount", IntegerType(), True),
    StructField("CargoWeight", DoubleType(), True)
])

print("‚úÖ Schemas defined successfully")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Configure Data Sources

# COMMAND ----------

# Configuration for data sources
config = {
    # Kafka/Event Hubs configuration
    "kafka": {
        "bootstrap.servers": "your-kafka-server:9092",
        "subscribe": "maritime.ais.data,maritime.environmental.sensors",
        "startingOffsets": "earliest",
        "kafka.security.protocol": "SASL_SSL",
        "kafka.sasl.mechanism": "PLAIN"
    },
    
    # Delta Lake paths
    "delta_lake": {
        "ais_data": "/mnt/maritime/delta/ais_positions",
        "environmental_data": "/mnt/maritime/delta/environmental_sensors",
        "voyage_data": "/mnt/maritime/delta/voyages",
        "alerts": "/mnt/maritime/delta/alerts"
    },
    
    # Data quality thresholds
    "quality": {
        "max_null_percentage": 5.0,
        "latitude_range": (-90, 90),
        "longitude_range": (-180, 180),
        "speed_range": (0, 40),  # knots
        "co2_threshold": 1000  # kg/hour
    }
}

print("‚úÖ Configuration loaded")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Data Quality Functions

# COMMAND ----------

def validate_ais_data(df):
    """
    Validate AIS vessel position data
    Returns: (valid_df, invalid_df)
    """
    # Check latitude/longitude ranges
    valid_df = df.filter(
        (col("Latitude").between(*config["quality"]["latitude_range"])) &
        (col("Longitude").between(*config["quality"]["longitude_range"])) &
        (col("Speed").between(*config["quality"]["speed_range"])) &
        (col("MMSI").isNotNull()) &
        (col("Timestamp").isNotNull())
    )
    
    invalid_df = df.exceptAll(valid_df)
    
    return valid_df, invalid_df

def validate_environmental_data(df):
    """
    Validate environmental sensor data
    Returns: (valid_df, invalid_df)
    """
    valid_df = df.filter(
        (col("CO2EmissionKg") >= 0) &
        (col("CO2EmissionKg") <= config["quality"]["co2_threshold"]) &
        (col("BatteryStateOfCharge").between(0, 100)) &
        (col("MeasurementTime").isNotNull())
    )
    
    invalid_df = df.exceptAll(valid_df)
    
    return valid_df, invalid_df

def add_metadata_columns(df):
    """
    Add metadata columns for tracking and partitioning
    """
    return df \
        .withColumn("ingestion_timestamp", current_timestamp()) \
        .withColumn("ingestion_date", current_date()) \
        .withColumn("year", year(col("Timestamp"))) \
        .withColumn("month", month(col("Timestamp"))) \
        .withColumn("day", dayofmonth(col("Timestamp")))

print("‚úÖ Data quality functions defined")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Batch Ingestion: AIS Historical Data

# COMMAND ----------

def ingest_ais_batch_data(source_path, target_delta_path):
    """
    Ingest historical AIS data in batch mode
    """
    print(f"üì• Starting batch ingestion of AIS data from {source_path}")
    
    # Read historical data (could be from CSV, Parquet, JSON, etc.)
    df_raw = spark.read \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .csv(source_path)
    
    # Convert to proper schema
    df_typed = df_raw.select(
        col("VesselName").cast(StringType()),
        col("MMSI").cast(StringType()),
        col("Latitude").cast(DoubleType()),
        col("Longitude").cast(DoubleType()),
        col("Speed").cast(DoubleType()),
        col("Heading").cast(IntegerType()),
        to_timestamp(col("Timestamp")).alias("Timestamp")
    )
    
    # Validate data quality
    df_valid, df_invalid = validate_ais_data(df_typed)
    
    # Log invalid records
    invalid_count = df_invalid.count()
    if invalid_count > 0:
        print(f"‚ö†Ô∏è  Found {invalid_count} invalid records")
        df_invalid.write \
            .mode("append") \
            .parquet(f"{target_delta_path}_invalid")
    
    # Add metadata
    df_final = add_metadata_columns(df_valid)
    
    # Write to Delta Lake with partitioning
    df_final.write \
        .format("delta") \
        .mode("append") \
        .partitionBy("year", "month", "day") \
        .option("mergeSchema", "true") \
        .save(target_delta_path)
    
    valid_count = df_valid.count()
    print(f"‚úÖ Successfully ingested {valid_count} AIS records")
    print(f"üìä Data quality: {(valid_count / (valid_count + invalid_count) * 100):.2f}% valid")
    
    return df_final

# Example usage (uncomment when ready)
# df_ais = ingest_ais_batch_data(
#     source_path="/mnt/maritime/raw/ais_history/*.csv",
#     target_delta_path=config["delta_lake"]["ais_data"]
# )

print("‚úÖ Batch ingestion function ready")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 6. Streaming Ingestion: Real-time from Kafka

# COMMAND ----------

def ingest_kafka_stream_ais():
    """
    Stream AIS data from Kafka to Delta Lake
    """
    print("üåä Starting Kafka stream ingestion for AIS data")
    
    # Read from Kafka
    df_stream = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", config["kafka"]["bootstrap.servers"]) \
        .option("subscribe", "maritime.ais.data") \
        .option("startingOffsets", "latest") \
        .load()
    
    # Parse JSON messages
    df_parsed = df_stream.select(
        col("key").cast(StringType()).alias("mmsi"),
        from_json(col("value").cast(StringType()), ais_schema).alias("data"),
        col("timestamp").alias("kafka_timestamp")
    ).select("data.*", "kafka_timestamp")
    
    # Add metadata
    df_enriched = df_parsed \
        .withColumn("ingestion_timestamp", current_timestamp()) \
        .withColumn("year", year(col("Timestamp"))) \
        .withColumn("month", month(col("Timestamp"))) \
        .withColumn("day", dayofmonth(col("Timestamp")))
    
    # Write to Delta Lake with checkpointing
    query = df_enriched.writeStream \
        .format("delta") \
        .outputMode("append") \
        .option("checkpointLocation", f"{config['delta_lake']['ais_data']}/_checkpoint") \
        .partitionBy("year", "month", "day") \
        .start(config["delta_lake"]["ais_data"])
    
    print(f"‚úÖ Streaming query started: {query.id}")
    return query

# Example usage (uncomment to start streaming)
# ais_stream_query = ingest_kafka_stream_ais()

print("‚úÖ Streaming ingestion function ready")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 7. Ingest Environmental Sensor Data

# COMMAND ----------

def ingest_environmental_batch_data(source_path, target_delta_path):
    """
    Ingest environmental sensor data in batch
    """
    print(f"üì• Starting batch ingestion of environmental data from {source_path}")
    
    df_raw = spark.read \
        .format("json") \
        .schema(environmental_schema) \
        .load(source_path)
    
    # Validate
    df_valid, df_invalid = validate_environmental_data(df_raw)
    
    # Add metadata and enrichment
    df_enriched = df_valid \
        .withColumn("ingestion_timestamp", current_timestamp()) \
        .withColumn("year", year(col("MeasurementTime"))) \
        .withColumn("month", month(col("MeasurementTime"))) \
        .withColumn("emission_category", 
            when(col("CO2EmissionKg") < 100, "Low")
            .when(col("CO2EmissionKg") < 500, "Medium")
            .otherwise("High")
        ) \
        .withColumn("compliance_status",
            when(col("CO2EmissionKg") <= 50, "Compliant")
            .otherwise("Review Required")
        )
    
    # Write to Delta
    df_enriched.write \
        .format("delta") \
        .mode("append") \
        .partitionBy("year", "month") \
        .save(target_delta_path)
    
    count = df_valid.count()
    print(f"‚úÖ Successfully ingested {count} environmental sensor records")
    
    return df_enriched

print("‚úÖ Environmental data ingestion function ready")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 8. Create Delta Tables and Optimize

# COMMAND ----------

def optimize_delta_tables():
    """
    Optimize Delta tables with Z-ordering and compaction
    """
    print("üîß Optimizing Delta tables...")
    
    # Optimize AIS data table
    spark.sql(f"""
        OPTIMIZE delta.`{config["delta_lake"]["ais_data"]}`
        ZORDER BY (MMSI, Timestamp)
    """)
    print("‚úÖ AIS data table optimized")
    
    # Optimize environmental data table
    spark.sql(f"""
        OPTIMIZE delta.`{config["delta_lake"]["environmental_data"]}`
        ZORDER BY (VesselId, MeasurementTime)
    """)
    print("‚úÖ Environmental data table optimized")
    
    # Vacuum old files (keep 7 days of history)
    spark.sql(f"""
        VACUUM delta.`{config["delta_lake"]["ais_data"]}` RETAIN 168 HOURS
    """)
    print("‚úÖ Delta tables vacuumed")

# Example usage
# optimize_delta_tables()

print("‚úÖ Optimization functions ready")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 9. Data Quality Monitoring

# COMMAND ----------

def generate_data_quality_report(delta_path, table_name):
    """
    Generate comprehensive data quality report
    """
    print(f"üìä Generating data quality report for {table_name}")
    
    df = spark.read.format("delta").load(delta_path)
    
    # Basic statistics
    total_records = df.count()
    
    # Null check for each column
    null_counts = {}
    for col_name in df.columns:
        null_count = df.filter(col(col_name).isNull()).count()
        null_percentage = (null_count / total_records * 100) if total_records > 0 else 0
        null_counts[col_name] = {"count": null_count, "percentage": null_percentage}
    
    # Duplicate check (if applicable)
    duplicates = df.count() - df.dropDuplicates().count()
    
    report = {
        "table_name": table_name,
        "total_records": total_records,
        "null_analysis": null_counts,
        "duplicate_count": duplicates,
        "date_range": df.agg(
            min("Timestamp").alias("earliest"),
            max("Timestamp").alias("latest")
        ).collect()[0].asDict() if "Timestamp" in df.columns else None,
        "generated_at": str(spark.sql("SELECT current_timestamp()").collect()[0][0])
    }
    
    # Print summary
    print(f"\n{'='*60}")
    print(f"Data Quality Report: {table_name}")
    print(f"{'='*60}")
    print(f"Total Records: {total_records:,}")
    print(f"Duplicate Records: {duplicates:,}")
    print(f"\nColumns with Nulls (>0%):")
    for col_name, stats in null_counts.items():
        if stats["percentage"] > 0:
            print(f"  - {col_name}: {stats['count']:,} ({stats['percentage']:.2f}%)")
    print(f"{'='*60}\n")
    
    return report

# Example usage
# quality_report = generate_data_quality_report(
#     delta_path=config["delta_lake"]["ais_data"],
#     table_name="AIS Positions"
# )

print("‚úÖ Data quality monitoring ready")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 10. Summary and Next Steps

# COMMAND ----------

print("""
‚úÖ Maritime Data Ingestion Pipeline Ready!

Pipeline Features:
==================
üì• Batch ingestion from various sources (CSV, JSON, Parquet)
üåä Streaming ingestion from Kafka/Event Hubs
‚úîÔ∏è  Comprehensive data quality validation
üóÇÔ∏è  Delta Lake storage with partitioning
üîß Table optimization with Z-ordering
üìä Data quality monitoring and reporting

Next Steps:
===========
1. Configure your data sources (Kafka, storage accounts)
2. Run batch ingestion for historical data
3. Start streaming queries for real-time data
4. Schedule optimization jobs
5. Set up data quality alerts

Delta Lake Tables Created:
==========================
- /mnt/maritime/delta/ais_positions
- /mnt/maritime/delta/environmental_sensors
- /mnt/maritime/delta/voyages
- /mnt/maritime/delta/alerts

Ready to process maritime data at scale! üö¢
""")

# COMMAND ----------